{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1.1: Install core Hugging Face libraries\n# This installs transformers, datasets, and accelerate\n!pip install transformers datasets accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:45:28.129066Z","iopub.execute_input":"2025-07-28T16:45:28.129611Z","iopub.status.idle":"2025-07-28T16:46:50.865509Z","shell.execute_reply.started":"2025-07-28T16:45:28.129585Z","shell.execute_reply":"2025-07-28T16:46:50.864799Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 1.3: Install the Hugging Face Evaluate library\n# This library is used for evaluating the model's performance\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:01:39.636565Z","iopub.execute_input":"2025-07-28T17:01:39.637341Z","iopub.status.idle":"2025-07-28T17:01:43.168147Z","shell.execute_reply.started":"2025-07-28T17:01:39.637308Z","shell.execute_reply":"2025-07-28T17:01:43.167399Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Import necessary Python modules","metadata":{}},{"cell_type":"code","source":"# Step 1.4: Import necessary Python modules\nimport torch                      # Core PyTorch library for tensors and neural networks\nimport numpy as np                # For numerical operations, especially with arrays\nfrom datasets import load_dataset # To load datasets from Hugging Face\nimport evaluate                   # *** CORRECTED: Import the 'evaluate' library itself ***\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer # Key classes for BERT, tokenization, and training\nimport gradio as gr               # For creating the interactive web demo\n\nprint(\"All necessary modules imported successfully! Step 1 should now be complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:02:12.747953Z","iopub.execute_input":"2025-07-28T17:02:12.748281Z","iopub.status.idle":"2025-07-28T17:02:57.847742Z","shell.execute_reply.started":"2025-07-28T17:02:12.748251Z","shell.execute_reply":"2025-07-28T17:02:57.846918Z"}},"outputs":[{"name":"stderr","text":"2025-07-28 17:02:31.452753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753722151.836361      80 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753722151.938706      80 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"All necessary modules imported successfully! Step 1 should now be complete.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load the AG News dataset","metadata":{}},{"cell_type":"code","source":"# Step 2.1: Load the AG News dataset\nprint(\"Loading AG News dataset (full version first)...\")\ndataset = load_dataset(\"ag_news\")\nprint(\"Full dataset loaded successfully!\")\n\n# Let's peek at the dataset structure to understand what we've loaded\nprint(\"\\nFull Dataset structure:\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:03:39.324725Z","iopub.execute_input":"2025-07-28T17:03:39.325459Z","iopub.status.idle":"2025-07-28T17:03:42.511779Z","shell.execute_reply.started":"2025-07-28T17:03:39.325423Z","shell.execute_reply":"2025-07-28T17:03:42.511188Z"}},"outputs":[{"name":"stdout","text":"Loading AG News dataset (full version first)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73fc404b43f4f698dd60f5073552185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2612f1e33c4c72af19aec79a750eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79c05e28bee4a95b899955790c9c5d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4915351f6226400e91395b480920e84b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05673dd23b2e43019973f42fdca555b8"}},"metadata":{}},{"name":"stdout","text":"Full dataset loaded successfully!\n\nFull Dataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Load the Tokenizer","metadata":{}},{"cell_type":"code","source":"# Step 2.2: Load the Tokenizer\nfrom transformers import AutoTokenizer\n\nmodel_name = \"bert-base-uncased\" # Sticking with BERT-base for now, remember DistilBERT if VRAM is still an issue\n\nprint(f\"\\nLoading tokenizer for {model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprint(\"Tokenizer loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:04:04.819608Z","iopub.execute_input":"2025-07-28T17:04:04.819918Z","iopub.status.idle":"2025-07-28T17:04:05.911724Z","shell.execute_reply.started":"2025-07-28T17:04:04.819892Z","shell.execute_reply":"2025-07-28T17:04:05.910952Z"}},"outputs":[{"name":"stdout","text":"\nLoading tokenizer for bert-base-uncased...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb73ab4aee5468a96089b988235e3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71bb73a330c4679bcee82de4c89eacb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10cc008a89147fc8cd2a51186c10078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb5df9c2bbf4c5a9bc6ddb5467b73a2"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Sample a Smaller Dataset","metadata":{}},{"cell_type":"code","source":"# Step 2.3: Sample a Smaller Dataset\ntrain_size = 10000\neval_size = 1000\n\nprint(f\"\\nCreating smaller dataset subsets: Training={train_size}, Evaluation={eval_size}...\")\n\n# Create a smaller training dataset\n# .shuffle(seed=42) ensures we get a random but reproducible subset\n# .select(range(train_size)) takes the first 'train_size' examples after shuffling\nsmall_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n\n# Create a smaller evaluation dataset\nsmall_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(eval_size))\n\nprint(\"Smaller datasets created successfully!\")\n\n# Let's inspect the size of our new datasets\nprint(f\"Size of small_train_dataset: {len(small_train_dataset)} samples\")\nprint(f\"Size of small_eval_dataset: {len(small_eval_dataset)} samples\")\n\n# And show a sample to confirm structure\nprint(\"\\nFirst example from small_train_dataset:\")\nprint(small_train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:04:31.061874Z","iopub.execute_input":"2025-07-28T17:04:31.062732Z","iopub.status.idle":"2025-07-28T17:04:31.078517Z","shell.execute_reply.started":"2025-07-28T17:04:31.062696Z","shell.execute_reply":"2025-07-28T17:04:31.077771Z"}},"outputs":[{"name":"stdout","text":"\nCreating smaller dataset subsets: Training=10000, Evaluation=1000...\nSmaller datasets created successfully!\nSize of small_train_dataset: 10000 samples\nSize of small_eval_dataset: 1000 samples\n\nFirst example from small_train_dataset:\n{'text': 'Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Bangladesh to a halt, the day after 18 people died in explosions at a political rally.', 'label': 0}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Define Tokenization Function","metadata":{}},{"cell_type":"code","source":"# Step 2.4: Define Tokenization Function\ndef tokenize_function(examples):\n    # The tokenizer processes the 'text' column from our dataset\n    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n\nprint(\"Tokenization function 'tokenize_function' defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:04:45.851542Z","iopub.execute_input":"2025-07-28T17:04:45.851824Z","iopub.status.idle":"2025-07-28T17:04:45.856454Z","shell.execute_reply.started":"2025-07-28T17:04:45.851803Z","shell.execute_reply":"2025-07-28T17:04:45.855580Z"}},"outputs":[{"name":"stdout","text":"Tokenization function 'tokenize_function' defined.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Apply Tokenization to Smaller Datasets","metadata":{}},{"cell_type":"code","source":"# Step 2.5: Apply Tokenization to Smaller Datasets\nprint(\"\\nApplying tokenization to the smaller datasets (this will be quick!)...\")\n\n# Apply the tokenization function to the small training dataset\ntokenized_small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n\n# Apply the tokenization function to the small evaluation dataset\ntokenized_small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n\nprint(\"Tokenization of smaller datasets complete!\")\n\n# Let's inspect the structure of the tokenized smaller datasets\nprint(\"\\nTokenized Small Training Dataset structure:\")\nprint(tokenized_small_train_dataset)\nprint(\"\\nFirst example from tokenized small training dataset:\")\nprint(tokenized_small_train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:05:02.691847Z","iopub.execute_input":"2025-07-28T17:05:02.692238Z","iopub.status.idle":"2025-07-28T17:05:05.490340Z","shell.execute_reply.started":"2025-07-28T17:05:02.692209Z","shell.execute_reply":"2025-07-28T17:05:05.489622Z"}},"outputs":[{"name":"stdout","text":"\nApplying tokenization to the smaller datasets (this will be quick!)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622044a9a08d48b497fcee6f1df6df86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46abfb4718764702b3ad986c91eed2a1"}},"metadata":{}},{"name":"stdout","text":"Tokenization of smaller datasets complete!\n\nTokenized Small Training Dataset structure:\nDataset({\n    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 10000\n})\n\nFirst example from tokenized small training dataset:\n{'text': 'Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Bangladesh to a halt, the day after 18 people died in explosions at a political rally.', 'label': 0, 'input_ids': [101, 7269, 11498, 2135, 6924, 2011, 9326, 4559, 10134, 2031, 2716, 2116, 4865, 1998, 3655, 1999, 7269, 2000, 1037, 9190, 1010, 1996, 2154, 2044, 2324, 2111, 2351, 1999, 18217, 2012, 1037, 2576, 8320, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Final Dataset Preparation for Trainer","metadata":{}},{"cell_type":"code","source":"# Step 2.6: Final Dataset Preparation for Trainer\nprint(\"\\nPreparing final datasets for the Trainer...\")\n\n# Remove original 'text' column and rename 'label' to 'labels'\n# The 'idx' column is NOT present by default in AG News, so we remove it from the list\ncolumns_to_remove = [\"text\"]\ntokenized_small_train_dataset = tokenized_small_train_dataset.remove_columns(columns_to_remove)\ntokenized_small_eval_dataset = tokenized_small_eval_dataset.remove_columns(columns_to_remove)\n\n# Rename 'label' to 'labels' as required by the Trainer\ntokenized_small_train_dataset = tokenized_small_train_dataset.rename_column(\"label\", \"labels\")\ntokenized_small_eval_dataset = tokenized_small_eval_dataset.rename_column(\"label\", \"labels\")\n\n# Set the format to PyTorch tensors\ntokenized_small_train_dataset.set_format(\"torch\")\ntokenized_small_eval_dataset.set_format(\"torch\")\n\nprint(\"Final datasets prepared for Trainer!\")\n\n# Confirm the new structure of the datasets\nprint(\"\\nFinal structure of tokenized_small_train_dataset:\")\nprint(tokenized_small_train_dataset)\nprint(\"\\nFirst example from final tokenized_small_train_dataset (showing tensor format):\")\nprint(tokenized_small_train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:05:35.459401Z","iopub.execute_input":"2025-07-28T17:05:35.459993Z","iopub.status.idle":"2025-07-28T17:05:35.484329Z","shell.execute_reply.started":"2025-07-28T17:05:35.459969Z","shell.execute_reply":"2025-07-28T17:05:35.483765Z"}},"outputs":[{"name":"stdout","text":"\nPreparing final datasets for the Trainer...\nFinal datasets prepared for Trainer!\n\nFinal structure of tokenized_small_train_dataset:\nDataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 10000\n})\n\nFirst example from final tokenized_small_train_dataset (showing tensor format):\n{'labels': tensor(0), 'input_ids': tensor([  101,  7269, 11498,  2135,  6924,  2011,  9326,  4559, 10134,  2031,\n         2716,  2116,  4865,  1998,  3655,  1999,  7269,  2000,  1037,  9190,\n         1010,  1996,  2154,  2044,  2324,  2111,  2351,  1999, 18217,  2012,\n         1037,  2576,  8320,  1012,   102,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0])}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Understanding Labels and Loading the Pre-trained Model","metadata":{}},{"cell_type":"code","source":"# Step 3.1: Understanding Labels and Loading the Pre-trained Model\nfrom transformers import AutoModelForSequenceClassification\nimport numpy as np\n\n# AG News has 4 classes: World, Sports, Business, Sci/Tech\n# The labels are usually 0, 1, 2, 3\nnum_labels = 4\n\n# Create mappings from ID to label name and vice versa for readability\nid2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\nlabel2id = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n\nprint(f\"\\nLoading {model_name} for sequence classification with {num_labels} labels...\")\n\n# Load the pre-trained BERT model with a classification head for 4 labels\n# We pass id2label and label2id to the model's configuration for better output interpretation\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n)\nprint(f\"Model {model_name} loaded successfully!\")\n\n# You'll likely see a warning about some weights not being initialized.\n# This is normal! The pre-trained model's original classification head is discarded,\n# and a new one is randomly initialized for your specific task (4 labels in our case).\n# This new head is what will be trained during fine-tuning.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:05:52.824401Z","iopub.execute_input":"2025-07-28T17:05:52.824684Z","iopub.status.idle":"2025-07-28T17:05:55.684696Z","shell.execute_reply.started":"2025-07-28T17:05:52.824661Z","shell.execute_reply":"2025-07-28T17:05:55.683940Z"}},"outputs":[{"name":"stdout","text":"\nLoading bert-base-uncased for sequence classification with 4 labels...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c1c6635e9f842ccb3903cdc4753e6ec"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model bert-base-uncased loaded successfully!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Define Training Arguments","metadata":{}},{"cell_type":"code","source":"# Step 3.2: Define Training Arguments\nfrom transformers import TrainingArguments\n\nprint(\"\\nSetting up Training Arguments\")\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",               # Directory where the trained model and checkpoints will be saved\n    eval_strategy=\"epoch\",          # Evaluate after each training epoch\n    learning_rate=2e-5,                   # Learning rate for the optimizer\n    per_device_train_batch_size=16,        # Optimized batch size for RAM management\n    per_device_eval_batch_size=16,         # Optimized batch size for RAM management\n    num_train_epochs=1,                   # <<< Set to 1 epoch for fastest training time\n    weight_decay=0.01,                    # L2 regularization to prevent overfitting\n    save_strategy=\"epoch\",                # Save a model checkpoint after each epoch\n    load_best_model_at_end=True,          # After training, load the best performing model based on evaluation metric\n    metric_for_best_model=\"f1\",           # Metric to use for determining the \"best\" model\n    report_to=\"none\",                     # Prevents reporting to external services like Weights & Biases if not configured\n)\nprint(\"Training Arguments set\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:08:19.284516Z","iopub.execute_input":"2025-07-28T17:08:19.284828Z","iopub.status.idle":"2025-07-28T17:08:19.318750Z","shell.execute_reply.started":"2025-07-28T17:08:19.284807Z","shell.execute_reply":"2025-07-28T17:08:19.317918Z"}},"outputs":[{"name":"stdout","text":"\nSetting up Training Arguments\nTraining Arguments set\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Defining compute_metrics","metadata":{}},{"cell_type":"code","source":"# Step 3.3: Defining compute_metrics\nimport evaluate # Import the evaluate library\nimport numpy as np # For numerical operations, especially argmax\n\n# Load the metrics we want to compute\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    Computes accuracy and F1 score from the model's predictions.\n    Args:\n        eval_pred (tuple): A tuple containing predictions (logits) and true labels.\n    Returns:\n        dict: A dictionary of computed metrics.\n    \"\"\"\n    logits, labels = eval_pred\n    # Get the predicted class by finding the index of the highest logit\n    predictions = np.argmax(logits, axis=-1)\n\n    # Compute accuracy\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n\n    # Compute F1 score (using 'weighted' average for multi-class to account for imbalance)\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n\n    # Return a dictionary of the results\n    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}\n\nprint(\"compute_metrics function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:08:42.362721Z","iopub.execute_input":"2025-07-28T17:08:42.363033Z","iopub.status.idle":"2025-07-28T17:08:43.402429Z","shell.execute_reply.started":"2025-07-28T17:08:42.362989Z","shell.execute_reply":"2025-07-28T17:08:43.401683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0a78f1fb5448b5bc02eb0106f67abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab2ece88f0c4acc94a4331bcdf92d39"}},"metadata":{}},{"name":"stdout","text":"compute_metrics function defined.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Initializing the Hugging Face Trainer","metadata":{}},{"cell_type":"code","source":"# Step 3.4: Initializing the Hugging Face Trainer\nfrom transformers import Trainer\nfrom transformers import DataCollatorWithPadding # Import DataCollatorWithPadding\n\n# A DataCollator is used to form batches by padding sequences to the longest length in that batch.\n# This makes training more efficient.\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nprint(\"\\nInitializing Hugging Face Trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_small_train_dataset, # Our smaller training dataset\n    eval_dataset=tokenized_small_eval_dataset,   # Our smaller evaluation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,                  # Use the data collator\n    compute_metrics=compute_metrics,\n)\nprint(\"Trainer initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:02.250376Z","iopub.execute_input":"2025-07-28T17:09:02.251131Z","iopub.status.idle":"2025-07-28T17:09:02.630813Z","shell.execute_reply.started":"2025-07-28T17:09:02.251104Z","shell.execute_reply":"2025-07-28T17:09:02.630066Z"}},"outputs":[{"name":"stdout","text":"\nInitializing Hugging Face Trainer...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_80/3318022645.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Trainer initialized successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Start the Model Training","metadata":{}},{"cell_type":"code","source":"# Step 3.5: Start the Model Training\nprint(\"\\nStarting model training...\")\n# This call executes the training loop\ntrainer.train()\nprint(\"Model training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:09:20.842547Z","iopub.execute_input":"2025-07-28T17:09:20.843139Z","iopub.status.idle":"2025-07-28T17:15:09.055712Z","shell.execute_reply.started":"2025-07-28T17:09:20.843111Z","shell.execute_reply":"2025-07-28T17:15:09.055109Z"}},"outputs":[{"name":"stdout","text":"\nStarting model training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 05:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.302441</td>\n      <td>0.902000</td>\n      <td>0.902046</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model training complete!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Save the Fine-tuned Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"# Step 3.6: Save the Fine-tuned Model and Tokenizer\noutput_model_dir = \"./fine_tuned_bert_agnews\"\n\nprint(f\"\\nSaving fine-tuned model and tokenizer to '{output_model_dir}'...\")\n# The Trainer's save_model() method saves both the model and its configuration\ntrainer.save_model(output_model_dir)\n# It's also good practice to explicitly save the tokenizer, though it's often saved with the model\ntokenizer.save_pretrained(output_model_dir)\n\nprint(\"Fine-tuned model and tokenizer saved successfully!\")\nprint(\"You can now load this model later using:\")\nprint(f\"  from transformers import AutoModelForSequenceClassification, AutoTokenizer\")\nprint(f\"  model = AutoModelForSequenceClassification.from_pretrained('{output_model_dir}')\")\nprint(f\"  tokenizer = AutoTokenizer.from_pretrained('{output_model_dir}')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:15:30.345356Z","iopub.execute_input":"2025-07-28T17:15:30.346126Z","iopub.status.idle":"2025-07-28T17:15:31.215487Z","shell.execute_reply.started":"2025-07-28T17:15:30.346100Z","shell.execute_reply":"2025-07-28T17:15:31.214650Z"}},"outputs":[{"name":"stdout","text":"\nSaving fine-tuned model and tokenizer to './fine_tuned_bert_agnews'...\nFine-tuned model and tokenizer saved successfully!\nYou can now load this model later using:\n  from transformers import AutoModelForSequenceClassification, AutoTokenizer\n  model = AutoModelForSequenceClassification.from_pretrained('./fine_tuned_bert_agnews')\n  tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_bert_agnews')\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Load the Saved Model and Tokenizer (Optional)","metadata":{}},{"cell_type":"code","source":"# Step 4.1: Load the Saved Model and Tokenizer (Optional)\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\noutput_model_dir = \"./fine_tuned_bert_agnews\" # Make sure this matches where you saved it\n\nprint(f\"\\nLoading the fine-tuned model and tokenizer from '{output_model_dir}' for evaluation...\")\nloaded_model = AutoModelForSequenceClassification.from_pretrained(output_model_dir)\nloaded_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\nprint(\"Model and tokenizer loaded successfully for evaluation!\")\n\n# Re-initialize the Trainer with the loaded model for final evaluation\n# We use the same training_args but only for evaluation purposes\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nimport evaluate # Make sure evaluate is imported if not already\nimport numpy as np\n\n# Re-define compute_metrics (if you've restarted your session)\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}\n\n# A DataCollator is used to form batches by padding sequences to the longest length in that batch.\ndata_collator = DataCollatorWithPadding(tokenizer=loaded_tokenizer)\n\n# Use dummy training args for evaluation if you don't need to retain the full training config\n# Or reuse your existing training_args, ensuring output_dir is where the model is loaded from\neval_training_args = TrainingArguments(\n    output_dir=\"./evaluation_results\", # A new temporary directory for evaluation logs\n    per_device_eval_batch_size=8,\n    report_to=\"none\",\n)\n\n\ntrainer = Trainer(\n    model=loaded_model,\n    args=eval_training_args, # Use args specifically for evaluation\n    eval_dataset=tokenized_small_eval_dataset, # Use the prepared evaluation dataset\n    tokenizer=loaded_tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:15:54.224627Z","iopub.execute_input":"2025-07-28T17:15:54.225404Z","iopub.status.idle":"2025-07-28T17:15:55.223818Z","shell.execute_reply.started":"2025-07-28T17:15:54.225376Z","shell.execute_reply":"2025-07-28T17:15:55.223069Z"}},"outputs":[{"name":"stdout","text":"\nLoading the fine-tuned model and tokenizer from './fine_tuned_bert_agnews' for evaluation...\nModel and tokenizer loaded successfully for evaluation!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_80/21247911.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Run Final Evaluation","metadata":{}},{"cell_type":"code","source":"# Step 4.2: Run Final Evaluation\nprint(\"\\nRunning final evaluation on the evaluation dataset...\")\n# The trainer already has the loaded_model and tokenized_small_eval_dataset\n# from the previous step's re-initialization.\nevaluation_results = trainer.evaluate()\n\nprint(\"\\n--- Final Evaluation Results ---\")\nfor metric_name, value in evaluation_results.items():\n    print(f\"{metric_name}: {value:.4f}\")\nprint(\"------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:16:25.504810Z","iopub.execute_input":"2025-07-28T17:16:25.505208Z","iopub.status.idle":"2025-07-28T17:16:34.896579Z","shell.execute_reply.started":"2025-07-28T17:16:25.505183Z","shell.execute_reply":"2025-07-28T17:16:34.895792Z"}},"outputs":[{"name":"stdout","text":"\nRunning final evaluation on the evaluation dataset...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n--- Final Evaluation Results ---\neval_loss: 0.3024\neval_accuracy: 0.9020\neval_f1: 0.9020\neval_runtime: 9.3815\neval_samples_per_second: 106.5930\neval_steps_per_second: 6.7150\n------------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Install Gradio and Define a Prediction Function","metadata":{}},{"cell_type":"code","source":"# Step 5.1: Install Gradio and Define a Prediction Function\n\n# Install Gradio (if not already installed)\ntry:\n    import gradio as gr\nexcept ImportError:\n    print(\"Gradio not found. Installing Gradio...\")\n    !pip install -q gradio\n    import gradio as gr\n    print(\"Gradio installed.\")\n\n# Make sure the model and tokenizer are loaded from the saved directory\n# If you just ran Step 4.1, they should already be in 'loaded_model' and 'loaded_tokenizer'\n# But it's good practice to re-load if you're running cells out of order or after a restart.\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch # Needed for moving tensors to CPU if on GPU\n\noutput_model_dir = \"./fine_tuned_bert_agnews\" # Make sure this matches where you saved it\n\nprint(f\"\\nLoading the fine-tuned model and tokenizer from '{output_model_dir}' for Gradio...\")\nmodel_for_inference = AutoModelForSequenceClassification.from_pretrained(output_model_dir)\ntokenizer_for_inference = AutoTokenizer.from_pretrained(output_model_dir)\nprint(\"Model and tokenizer loaded for inference!\")\n\n# Ensure model is on CPU if you're running locally without sufficient VRAM,\n# or if you want to explicitly keep it off the GPU for Gradio.\n# If running in Colab with GPU, it's fine to keep it on 'cuda'\nif torch.cuda.is_available() and model_for_inference.device.type == 'cuda':\n    print(f\"Model currently on GPU: {model_for_inference.device}. Moving to CPU for consistent inference...\")\n    model_for_inference.to('cpu')\n    print(\"Model moved to CPU.\")\n\n\n# Define the prediction function for Gradio\ndef classify_news_topic(text):\n    # 1. Tokenize the input text\n    inputs = tokenizer_for_inference(text, return_tensors=\"pt\", padding=True, truncation=True)\n\n    # 2. Perform inference\n    # Ensure inputs are on the same device as the model (CPU in this case, if moved)\n    with torch.no_grad():\n        outputs = model_for_inference(**inputs)\n\n    # 3. Get predicted class (logits to probabilities to class ID)\n    logits = outputs.logits\n    predicted_class_id = torch.argmax(logits, dim=-1).item()\n\n    # 4. Map class ID to human-readable label\n    # We need the id2label mapping, which was stored in the model's config\n    # You can access it directly from the loaded model\n    predicted_label = model_for_inference.config.id2label[predicted_class_id]\n\n    # Optional: Get confidence score\n    probabilities = torch.softmax(logits, dim=-1)[0]\n    confidence = probabilities[predicted_class_id].item()\n\n    return f\"Predicted Topic: {predicted_label} (Confidence: {confidence:.2f})\"\n\nprint(\"classify_news_topic function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:16:56.336840Z","iopub.execute_input":"2025-07-28T17:16:56.337663Z","iopub.status.idle":"2025-07-28T17:16:56.446922Z","shell.execute_reply.started":"2025-07-28T17:16:56.337639Z","shell.execute_reply":"2025-07-28T17:16:56.446202Z"}},"outputs":[{"name":"stdout","text":"\nLoading the fine-tuned model and tokenizer from './fine_tuned_bert_agnews' for Gradio...\nModel and tokenizer loaded for inference!\nclassify_news_topic function defined.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Launch the Gradio Interface","metadata":{}},{"cell_type":"code","source":"# Step 5.2: Launch the Gradio Interface\nprint(\"\\nLaunching Gradio Interface...\")\n\n# Create the Gradio Interface\niface = gr.Interface(\n    fn=classify_news_topic,\n    inputs=gr.Textbox(lines=5, placeholder=\"Enter news text here...\", label=\"News Headline/Article\"),\n    outputs=gr.Textbox(label=\"Predicted News Topic\"),\n    title=\"AG News Topic Classifier\",\n    description=\"Enter a news headline or short article to classify its topic (World, Sports, Business, Sci/Tech).\",\n    live=True, # Update predictions as you type\n)\n\n# Launch the interface\n# For Kaggle and Colab, `share=True` is necessary to get a public URL\niface.launch(share=True)\n\nprint(\"Gradio Interface launched!\")\nprint(\"Look for a public URL (e.g., 'https://xxxx.gradio.live') above this cell after execution.\")\nprint(\"Click on that URL to interact with your model!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:17:24.585257Z","iopub.execute_input":"2025-07-28T17:17:24.585552Z","iopub.status.idle":"2025-07-28T17:17:26.519368Z","shell.execute_reply.started":"2025-07-28T17:17:24.585531Z","shell.execute_reply":"2025-07-28T17:17:26.518469Z"}},"outputs":[{"name":"stdout","text":"\nLaunching Gradio Interface...\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://00583c8e770796a0f1.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://00583c8e770796a0f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Gradio Interface launched!\nLook for a public URL (e.g., 'https://xxxx.gradio.live') above this cell after execution.\nClick on that URL to interact with your model!\n","output_type":"stream"}],"execution_count":22}]}