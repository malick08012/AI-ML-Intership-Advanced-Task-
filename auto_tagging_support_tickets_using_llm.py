# -*- coding: utf-8 -*-
"""Auto Tagging Support Tickets Using LLM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jw2hsiO28scr-FmYDPISfXFQl_MYAMsD
"""

from google.colab import drive
drive.mount('/content/drive')

"""#1. Load the Dataset
 We'll use the pandas library to load it, which is excellent for handling tabular data like CSVs.
"""

import pandas as pd

# Load the customer support tickets CSV file into a DataFrame
df = pd.read_csv('customer_support_tickets.csv')

# Display the first few rows of the dataset to understand its structure
print("Dataset Head (First 5 Rows):")
print(df.head())

# Display general information about the dataset, like column names and data types
print("\nDataset Information:")
df.info()

"""# 2. Clean Data and Rename Columns
In this step we handling any missing values in the critical columns and renaming them to text and tags. This will ensure our data is in the correct format for the LLM processing steps that follow.
"""

# Drop rows where 'Ticket Description' or 'Ticket Subject' are missing,
# as these columns are essential for our task.
initial_rows = len(df)
df.dropna(subset=['Ticket Description', 'Ticket Subject'], inplace=True)
print(f"Removed {initial_rows - len(df)} rows with missing 'Ticket Description' or 'Ticket Subject'.")
print(f"Remaining rows: {len(df)}")

# --- IMPORTANT FIX HERE: Also rename 'Ticket ID' to 'ticket_id' ---
df.rename(columns={
    'Ticket Description': 'text',
    'Ticket Subject': 'tags',
    'Ticket ID': 'ticket_id' # Renaming the Ticket ID column
}, inplace=True)

# Display the dataset head again to confirm changes
print("\nDataset Head after cleaning and renaming:")
print(df.head())

# Show the unique tags and their distribution to understand our categories
print("\nUnique Tags (Categories) and their Distribution:")
print(df['tags'].value_counts())
print(f"\nTotal number of unique tags: {len(df['tags'].unique())}")

"""#3. Split the Dataset
1. Training Set: Used to train our LLM. The model learns patterns from this data.

2. Validation Set: Used during the training process to monitor performance and make decisions about model tuning (e.g., how many epochs to train). This helps prevent overfitting.

3. Test Set: A completely separate set of data, unseen by the model during      training or validation, used for a final, unbiased evaluation of the model's performance.
"""

from sklearn.model_selection import train_test_split

# Define the split ratios: 70% for training, 15% for validation, 15% for testing.
# temp_df will hold the combined validation and test sets (30% of total)
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['tags'])

# Split the temp_df equally into validation and test sets (15% each)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['tags'])

print(f"Dataset split complete:")
print(f"  Training set size: {len(train_df)} samples")
print(f"  Validation set size: {len(val_df)} samples")
print(f"  Test set size: {len(test_df)} samples")

print("\nTag distribution in Training set:")
print(train_df['tags'].value_counts())

print("\nTag distribution in Validation set:")
print(val_df['tags'].value_counts())

print("\nTag distribution in Test set:")
print(test_df['tags'].value_counts())

"""#4. LLM Setup and Tokenization

1. Loading the Tokenizer:  Every pre-trained LLM understands text by converting it into numerical tokens. We need to load the specific tokenizer that corresponds to our chosen LLM.

2. Creating Label Mappings: Our tags (like "Network problem") are human-readable strings. The LLM, when trained for classification, will output numerical IDs (e.g., 0, 1). We need to create mappings (dictionaries) to convert between these string labels and numerical IDs.
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Choose a model. 'distilbert-base-uncased' is a good, lightweight choice for fine-tuning.
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define our labels based on the unique tags from the entire dataset (df)
# The 'tags' column was created in Step 2.
unique_tags = df['tags'].unique().tolist()
unique_tags.sort() # Sorting ensures consistent mapping for reproducibility

id2label = {idx: label for idx, label in enumerate(unique_tags)}
label2id = {label: idx for label, idx in enumerate(unique_tags)} # label:idx for label in list, not for idx in enumerate(unique_tags)

# Corrected mapping creation:
label2id = {label: idx for idx, label in enumerate(unique_tags)}
id2label = {idx: label for idx, label in enumerate(unique_tags)}


print(f"Model selected for fine-tuning: {model_name}")
print(f"Unique labels (tags): {unique_tags}")
print(f"Number of unique labels: {len(unique_tags)}")
print(f"Label to ID mapping (label2id): {label2id}")
print(f"ID to Label mapping (id2label): {id2label}")


# Define the function to tokenize text. This will be applied to our datasets later.
def tokenize_function(examples):
    # Ensure 'text' column exists in examples dictionary
    if 'text' not in examples:
        raise ValueError("The input 'examples' dictionary must contain a 'text' key.")
    # Use the tokenizer to convert text into input IDs and attention masks.
    # truncation=True: Truncates longer sequences to max_length.
    # padding="max_length": Pads shorter sequences to max_length.
    # max_length=128: Sets the maximum sequence length. Adjust based on typical ticket length.
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

print("\nTokenizer loaded and label mappings created successfully.")
print("The 'tokenize_function' is also defined, ready for use.")

"""#Result:

1. model_name = "distilbert-base-uncased": We specify which pre-trained model we want to use. DistilBERT is a smaller, faster version of BERT, good for starting out.

2. tokenizer = AutoTokenizer.from_pretrained(model_name): This line loads the specific tokenizer for DistilBERT. It's like a dictionary that converts words/subwords into numerical tokens that the model understands.

#5. Prepare Datasets for LLM (Tokenization and Formatting)

 This step is to convert our pandas DataFrames (train_df, val_df, test_df) into a format that the Hugging Face Trainer can easily work with.
"""

from datasets import Dataset # Hugging Face's Dataset library

# Convert pandas DataFrames to Hugging Face Dataset objects
# Rename 'tags' column to 'label' as expected by standard Hugging Face classification models
train_dataset = Dataset.from_pandas(train_df.rename(columns={'tags': 'label'}))
val_dataset = Dataset.from_pandas(val_df.rename(columns={'tags': 'label'}))
test_dataset_ft = Dataset.from_pandas(test_df.rename(columns={'tags': 'label'})) # Distinct name for test_df for fine-tuning context

# Map string labels to integers for fine-tuning
# The label2id mapping was created in Section 4
def map_labels_to_ids(example):
    # Ensure the label exists in our mapping to prevent KeyError
    if example['label'] in label2id:
        example['label'] = label2id[example['label']]
    else:
        # Handle cases where a label might not be in the mapping (e.g., if unique_tags was from subset)
        # For this task, it should always be in, but good practice for robustness
        print(f"Warning: Label '{example['label']}' not found in label2id mapping. Skipping.")
        example['label'] = -1 # Assign an invalid ID or handle as appropriate
    return example

print("Mapping string labels to integer IDs for datasets...")
train_dataset = train_dataset.map(map_labels_to_ids)
val_dataset = val_dataset.map(map_labels_to_ids)
test_dataset_ft = test_dataset_ft.map(map_labels_to_ids)


# Tokenize datasets using the tokenize_function from Section 4
print("Tokenizing datasets...")
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)
tokenized_test_dataset_ft = test_dataset_ft.map(tokenize_function, batched=True)

# Remove original 'text' column and any pandas-specific index column ('__index_level_0__')
# as they are not directly used by the model during training.
print("Cleaning up dataset columns for model input...")
tokenized_train_dataset = tokenized_train_dataset.remove_columns(["text", "__index_level_0__"])
tokenized_val_dataset = tokenized_val_dataset.remove_columns(["text", "__index_level_0__"])
tokenized_test_dataset_ft = tokenized_test_dataset_ft.remove_columns(["text", "__index_level_0__"])

# Set the format for PyTorch, which is commonly used with Hugging Face models
tokenized_train_dataset.set_format("torch")
tokenized_val_dataset.set_format("torch")
tokenized_test_dataset_ft.set_format("torch")

print("\nDatasets are now tokenized and formatted for LLM training and evaluation.")
print(f"Example of tokenized_train_dataset feature names: {tokenized_train_dataset.column_names}")

"""#6. Zero-Shot Learning

1. This method allows an LLM to categorize text without any prior training on your specific dataset. It relies on the model's general understanding of language and its ability to infer the relationship between your ticket text and the provided category labels.

2. We'll use a pre-trained model designed for Natural Language Inference (NLI), like facebook/bart-large-mnli, which can determine if a "hypothesis" (our label) is true given a "premise" (our ticket text).
"""

from transformers import pipeline
import pandas as pd
import numpy as np
import torch # Ensure torch is imported for device check

print("--- Starting Zero-Shot Learning ---")

# Load a zero-shot classification pipeline.
# 'facebook/bart-large-mnli' is a common choice for zero-shot text classification.
zero_shot_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=0 if torch.cuda.is_available() else -1)

print(f"Zero-Shot Classifier loaded: {zero_shot_classifier.model.config._name_or_path}")
print(f"Running on device: {'cuda' if torch.cuda.is_available() else 'cpu'}")

# Use the unique_tags defined in Step 4 as candidate labels
candidate_labels = unique_tags

print(f"\nUsing {len(candidate_labels)} candidate labels for zero-shot: {candidate_labels}")

print("\n--- Zero-Shot Predictions on Test Set (Limited Samples for Speed) ---")
zero_shot_predictions = {}
correct_zero_shot = 0
total_predictions_zero_shot = 0

num_samples_to_process = 50 # Process only the first 50 samples from the test set
print(f"Processing only the first {num_samples_to_process} samples from the test set for demonstration.")

# Iterate through a limited number of samples in the test set
for idx, row in test_df.head(num_samples_to_process).iterrows(): # Use .head() to limit
    ticket_id = row['ticket_id']
    text = row['text']
    actual_tag = row['tags']

    # Perform zero-shot classification
    result = zero_shot_classifier(text, candidate_labels=candidate_labels, multi_label=False)

    predicted_labels_with_scores = list(zip(result['labels'], result['scores']))
    top_3_predictions = predicted_labels_with_scores[:3]

    zero_shot_predictions[ticket_id] = {'text': text, 'actual_tag': actual_tag, 'predictions': top_3_predictions}

    print(f"Ticket ID: {ticket_id}")
    print(f"Text: {text}")
    print(f"Actual Tag: {actual_tag}")
    print(f"Predicted (Zero-Shot, Top 3): {top_3_predictions}\n")

    total_predictions_zero_shot += 1
    if top_3_predictions and top_3_predictions[0][0] == actual_tag:
        correct_zero_shot += 1

zero_shot_accuracy = correct_zero_shot / total_predictions_zero_shot if total_predictions_zero_shot > 0 else 0
print(f"Zero-Shot Top-1 Accuracy (on {num_samples_to_process} samples): {zero_shot_accuracy:.4f}")

print("\n--- Zero-Shot Learning Complete ---")

"""# Result:

For the purpose of quickly seeing the output and understanding the concept, we  modify the Zero-Shot step to process only a smaller subset of test data. This will significantly reduce the execution time.

So we only work on 50 Samples

#7. Few-Shot Learning

1. This method enhances the LLM's classification ability by providing it with a few examples of input-output pairs (ticket text and its correct tag) directly within the prompt. The model then learns from these examples how to classify new, unseen tickets.

2. For Few-Shot learning, generative models like google/flan-t5-small are often used because they are good at following instructions and generating text based on a given context, which is perfect for this prompt-based approach.
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM # Use Seq2SeqLM for T5 generative
import torch
import pandas as pd # Ensure pandas is imported as we use DataFrames
import numpy as np # Ensure numpy is imported

# For few-shot, we'll generally use a model that excels at instruction following.
# 'google/flan-t5-small' is a good choice for this type of task.
few_shot_model_name = "google/flan-t5-small"
few_shot_tokenizer = AutoTokenizer.from_pretrained(few_shot_model_name)

# Load as a Seq2SeqLM for generative tasks (T5 models are sequence-to-sequence)
few_shot_model = AutoModelForSeq2SeqLM.from_pretrained(few_shot_model_name)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
few_shot_model.to(device)

print(f"Using Few-Shot Generative Model: {few_shot_model_name} on device: {device}")

# Prepare few-shot examples from the training data
num_few_shot_examples = 5 # Number of examples to include in the prompt

# Get a stratified sample from train_df to ensure diverse examples
# Try to pick one example per class if possible, up to num_few_shot_examples
few_shot_examples = pd.DataFrame()
if len(unique_tags) > 0: # Ensure there are unique tags
    n_per_class_target = max(1, num_few_shot_examples // len(unique_tags))
    for tag in unique_tags:
        # Get samples for the current tag, up to n_per_class_target, but not exceeding available
        samples = train_df[train_df['tags'] == tag].sample(
            n=min(n_per_class_target, len(train_df[train_df['tags'] == tag])),
            random_state=42 # for reproducibility
        )
        few_shot_examples = pd.concat([few_shot_examples, samples])
    # If we picked more than num_few_shot_examples due to small num_few_shot_examples // len(unique_tags),
    # then randomly select down to the desired number.
    few_shot_examples = few_shot_examples.sample(n=min(num_few_shot_examples, len(few_shot_examples)), random_state=42)
else:
    # Fallback if no unique tags (shouldn't happen with valid data)
    few_shot_examples = train_df.sample(n=min(num_few_shot_examples, len(train_df)), random_state=42)

print(f"\nSelected {len(few_shot_examples)} few-shot examples for the prompt:")
for idx, row in few_shot_examples.iterrows():
    print(f"- Ticket: \"{row['text'][:50]}...\" | Tag: {row['tags']}")


def create_few_shot_prompt(ticket_text, examples_df, candidate_labels):
    """
    Constructs a prompt for few-shot learning for a generative model.
    Args:
        ticket_text (str): The support ticket text to classify.
        examples_df (pd.DataFrame): DataFrame containing few-shot examples (text, tags).
        candidate_labels (list): A list of all possible categories.
    Returns:
        str: The constructed prompt.
    """
    prompt = "Categorize the following support tickets into one of these specific categories: " + ", ".join(candidate_labels) + ".\n\n"
    prompt += "Examples:\n"
    for idx, row in examples_df.iterrows():
        prompt += f"Ticket: {row['text']}\nCategory: {row['tags']}\n\n"
    prompt += f"Classify the following:\nTicket: {ticket_text}\nCategory:"
    return prompt

def few_shot_generate_predict(text, examples_df, candidate_labels, max_new_tokens=10):
    """
    Performs few-shot classification using a generative model and parses its output.
    Returns a list of (label, score) tuples for top predictions.
    """
    prompt = create_few_shot_prompt(text, examples_df, candidate_labels)

    inputs = few_shot_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device) # Limit prompt length

    few_shot_model.eval() # Set model to evaluation mode
    with torch.no_grad(): # Disable gradient calculations for inference
        # Generate output using beam search for better quality
        generated_ids = few_shot_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=5, # Use beam search for better results
            early_stopping=True,
            no_repeat_ngram_size=2 # Helps avoid repetitive outputs
        )

    # Access the tensor directly instead of .sequences
    generated_text = few_shot_tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()

    # --- Post-processing for Top-3 (Heuristic for Generative Models) ---
    predicted_labels_with_scores = []
    found_exact_match = False
    for label in candidate_labels:
        if label.lower() == generated_text.lower() and not found_exact_match: # Exact match for highest confidence
            predicted_labels_with_scores.append((label, 1.0))
            found_exact_match = True
            break # Only take the first exact match

    # If no exact match found, but generated_text contains a candidate label as a substring
    if not found_exact_match and generated_text:
        potential_matches = []
        for label in candidate_labels:
            if label.lower() in generated_text.lower() or generated_text.lower() in label.lower():
                potential_matches.append((label, 0.9)) # Assign a moderate score for partial match
        # Sort potential matches by how well the generated text matches the label string
        # (e.g., longer common substring, or exact match first, then partial)
        # For simplicity, sorting by score and then length of label
        predicted_labels_with_scores.extend(sorted(potential_matches, key=lambda x: (x[1], len(x[0])), reverse=True))


    # Fill up to 3 predictions. If we have fewer than 3, fill with other random labels from candidates
    current_predicted_labels = {item[0] for item in predicted_labels_with_scores}
    remaining_candidates = [label for label in candidate_labels if label not in current_predicted_labels]
    np.random.shuffle(remaining_candidates) # Shuffle to avoid bias

    for i in range(min(3 - len(predicted_labels_with_scores), len(remaining_candidates))):
        predicted_labels_with_scores.append((remaining_candidates[i], 0.0)) # Dummy low score for filler

    # Sort by score if any valid scores were assigned, otherwise just use the filler order.
    predicted_labels_with_scores = sorted(predicted_labels_with_scores, key=lambda x: x[1], reverse=True)[:3]

    return predicted_labels_with_scores


print("\n--- Few-Shot Predictions on Test Set")
few_shot_predictions = {}
correct_few_shot = 0
total_predictions_few_shot = 0

# Use the same limit as Zero-Shot for consistency in comparison
num_samples_to_process = 50
print(f"Processing only the first {num_samples_to_process} samples from the test set for demonstration.")

for idx, row in test_df.head(num_samples_to_process).iterrows(): # Use .head() to limit
    ticket_id = row['ticket_id']
    text = row['text']
    actual_tag = row['tags']

    predictions = few_shot_generate_predict(text, few_shot_examples, candidate_labels)
    few_shot_predictions[ticket_id] = {'text': text, 'actual_tag': actual_tag, 'predictions': predictions}

    print(f"Ticket ID: {ticket_id}")
    print(f"Text: {text}")
    print(f"Actual Tag: {actual_tag}")
    print(f"Predicted (Few-Shot): {predictions}\n")

    total_predictions_few_shot += 1
    # Check top-1 accuracy based on the first predicted label
    if predictions and predictions[0][0] == actual_tag:
        correct_few_shot += 1

few_shot_accuracy = correct_few_shot / total_predictions_few_shot if total_predictions_few_shot > 0 else 0
print(f"Few-Shot Top-1 Accuracy (on {num_samples_to_process} samples): {few_shot_accuracy:.4f}")

print("\n--- Few-Shot Learning Complete ---")

"""# Result:

1. Loads Generative Model: It loads google/flan-t5-small using AutoModelForSeq2SeqLM, which is designed for text generation based on prompts.

2. Selects Few-Shot Examples: It samples a small number (num_few_shot_examples, currently 5) of examples from your train_df. These examples will be inserted directly into the prompt to guide the model. It tries to get a stratified sample for better representation.

3. create_few_shot_prompt Function: This function constructs the full prompt for the LLM. It includes clear instructions, the selected few-shot examples (each showing "Ticket: [text]\nCategory: [tag]"), and finally the new ticket to classify, ending with "Category:".

# 8. Fine-Tuning an LLM for Text Classification

In this method, we take a pre-trained LLM (like DistilBERT) and continue to train it on your labeled support ticket dataset. This allows the model to learn the specific nuances, vocabulary, and patterns related to your categories, often leading to significantly higher accuracy compared to zero-shot or few-shot methods.

We'll use the Hugging Face Trainer API, which simplifies the training loop.
"""

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np
import torch # Ensure torch is imported

# Load the model for sequence classification
# We're using the same 'model_name' ('distilbert-base-uncased') as for the tokenizer in Step 4
# We need to pass the number of labels and our id2label/label2id mappings.
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(unique_tags),
    id2label=id2label,
    label2id=label2id
)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(f"Fine-tuning model loaded: {model_name} with {len(unique_tags)} labels.")
print(f"Running on device: {device}")

# Define training arguments
# These control various aspects of the training process
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1, # <--- REDUCED EPOCHS FOR QUICKER RUN
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    fp16=True, # <--- ADD THIS LINE FOR MIXED PRECISION
    report_to="none"
)

# Define metrics computation function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    # Calculate desired metrics
    accuracy = accuracy_score(labels, predictions)
    # Using 'weighted' average for f1, precision, recall to account for class imbalance
    f1 = f1_score(labels, predictions, average="weighted")
    precision = precision_score(labels, predictions, average="weighted", zero_division=0)
    recall = recall_score(labels, predictions, average="weighted", zero_division=0)

    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

print("\nTraining arguments and compute_metrics function defined.")

# Initialize the Trainer
# tokenized_train_dataset and tokenized_val_dataset were created in Step 5
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    tokenizer=tokenizer, # Use the tokenizer loaded in Step 4
    compute_metrics=compute_metrics,
)

print("\nTrainer initialized. Starting training...")

# Train the model
trainer.train()

print("\nFine-tuning training complete. Evaluating on the test set...")

# Evaluate the model on the test set
# tokenized_test_dataset_ft was created in Step 5
fine_tuned_test_results = trainer.evaluate(tokenized_test_dataset_ft)

print(f"\nFine-tuned Model Test Results: {fine_tuned_test_results}")

print("\n--- LLM Fine-Tuning Complete ---")

"""# Result:

Training Large Language Models can indeed be time-consuming. Here we use the most effective ways that we can save time during the fine-tuning process:

1.  In TrainingArguments, change num_train_epochs to a smaller number, like 1

2. In TrainingArguments, add fp16=True. This requires a compatible GPU (most modern NVIDIA GPUs are, and Colab's GPUs generally support it).

3. Reduce the Maximum Sequence Length (max_length)

# 9. Compare Performance Across Approaches

Now that we've run all three LLM approaches (Zero-Shot, Few-Shot, and Fine-Tuning), the final step is to bring their results together and compare their performance. This will help us understand which method is most effective for your support ticket classification task.
"""

NUM_SAMPLES_FOR_DEMO = 50 # Number of samples used for Zero-Shot and Few-Shot demos
# NUM_FINE_TUNE_EPOCHS = 1 # Not directly needed for this print, but for context
# FINE_TUNE_BATCH_SIZE = 16 # Not directly needed for this print, but for context
# MAX_SEQUENCE_LENGTH = 128 # Not directly needed for this print, but for context

print("\n--- Comparing Performance Across Approaches ---")

# Ensure all accuracy variables are defined from previous steps
# Zero-Shot accuracy (from Step 6)
if 'zero_shot_accuracy' in locals():
    print(f"Zero-Shot Top-1 Accuracy (on {NUM_SAMPLES_FOR_DEMO} samples): {zero_shot_accuracy:.4f}")
else:
    print("Zero-Shot accuracy not available. Please ensure Step 6 was run.")

# Few-Shot accuracy (from Step 7)
if 'few_shot_accuracy' in locals():
    print(f"Few-Shot Top-1 Accuracy (on {NUM_SAMPLES_FOR_DEMO} samples): {few_shot_accuracy:.4f}")
else:
    print("Few-Shot accuracy not available. Please ensure Step 7 was run.")

# Fine-tuned model accuracy (from Step 8)
if 'fine_tuned_test_results' in locals() and 'eval_accuracy' in fine_tuned_test_results:
    print(f"Fine-tuned Model Test Accuracy: {fine_tuned_test_results['eval_accuracy']:.4f}")
elif 'fine_tuned_test_results' in locals():
    print(f"Fine-tuned Model Test Results (full, accuracy not directly found): {fine_tuned_test_results}")
else:
    print("Fine-tuned model results not available. Please ensure Step 8 was run.")

print("\n--- All Auto-Tagging Processes Complete ---")
print("You can now review the accuracies to determine the best approach for your specific needs.")

"""# Result:

Here are the performance results for each of the auto-tagging approaches:

1. Zero-Shot Learning: Top-1 Accuracy of 6.00% (on 50 samples)

2. Few-Shot Learning: Top-1 Accuracy of 6.00% (on 50 samples)

3. Fine-tuned Model: Test Accuracy of 6.45%

Based on these results, the Fine-tuned Model showed a slightly higher accuracy compared to Zero-Shot and Few-Shot learning on dataset. This often indicates that providing the model with direct training on your data helps it better understand and classify your support tickets.

# Final Summary and Insights:

In general, an accuracy of around 6% is not considered good for a classification task, especially with an LLM. Since you have 16 unique tags, random guessing would give you an accuracy of approximately 1/16, which is about 6.25%. This means the models are performing only slightly better.

#Possible reasons for the low accuracy:

1. Limited Data/Examples: For Zero-Shot and Few-Shot, we only used 50 samples for the demonstration. Running on the full test set might give a slightly different picture, but the core limitation of these methods for specialized tasks often remains.

2. Limited Fine-Tuning: The fine-tuning was performed for only 1 epoch to save time, and on a relatively small model (distilbert-base-uncased). LLMs often require more epochs or larger models to learn complex patterns and achieve higher performance.

#Steps for Improvement:

1. Increase Fine-Tuning Epochs: Try setting NUM_FINE_TUNE_EPOCHS to 3 or 5 in the fine-tuning step. This will take longer, but it's likely to improve accuracy significantly.

2. Experiment with Different Models: For fine-tuning, you could try a slightly larger or different pre-trained model (e.g., bert-base-uncased if DistilBERT isn't sufficient, though it will take more time/resources).

3. More Few-Shot Examples: For Few-Shot, try increasing num_few_shot_examples if your prompt length allows, or carefully select diverse examples.

4. Data Augmentation: If you have limited training data, techniques like data augmentation (creating synthetic examples) could help.

#Importance of Iteration and Resources:

1. More Training Epochs: Running the fine-tuning for more epochs (e.g., 3-5) is typically crucial for the model to fully learn the patterns.

2. Larger Models: For more complex classification, larger base models might be necessary, though they demand more computational resources (GPU memory, time).

3. Hyperparameter Tuning: Optimizing parameters like learning rate, batch size, etc., can significantly impact performance.

4. Data Quality and Quantity: The inherent quality and representativeness of the labeled dataset are paramount. If ticket descriptions are short, vague, or ambiguously tagged, even the best LLM will struggle.


This project provided a practical walkthrough of implementing LLM-based auto-tagging. While the initial observed accuracies were low, the framework is robust, demonstrating the different methodologies and providing clear avenues for further optimization to achieve production-level performance.
"""